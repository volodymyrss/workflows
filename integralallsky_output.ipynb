{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ab200a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa9a18c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:38.477691Z",
     "iopub.status.busy": "2023-05-18T15:53:38.477103Z",
     "iopub.status.idle": "2023-05-18T15:53:43.810518Z",
     "shell.execute_reply": "2023-05-18T15:53:43.809535Z"
    },
    "papermill": {
     "duration": 5.349676,
     "end_time": "2023-05-18T15:53:43.813198",
     "exception": false,
     "start_time": "2023-05-18T15:53:38.463522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to get AUTH with env due to: 'K8S_SECRET_INTEGRAL_CLIENT_SECRET'\n",
      "got AUTH with homefile for integral-limited\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "from matplotlib import pylab\n",
    "from astropy.coordinates import SkyCoord\n",
    "import integralclient as ic\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "from scipy import stats\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abfa4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:43.836781Z",
     "iopub.status.busy": "2023-05-18T15:53:43.836075Z",
     "iopub.status.idle": "2023-05-18T15:53:43.848161Z",
     "shell.execute_reply": "2023-05-18T15:53:43.847258Z"
    },
    "papermill": {
     "duration": 0.026815,
     "end_time": "2023-05-18T15:53:43.852048",
     "exception": false,
     "start_time": "2023-05-18T15:53:43.825233",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RA=293.732\n",
    "Dec=21.8967222\n",
    "tstart_rel_mseconds=300.0\n",
    "tstop_rel_seconds=300.0\n",
    "t0_utc=\"2023-05-18T12:59:08.000000\" # hard x-ray\n",
    "# t0_utc=Time(Time(\"2022-10-14T19:21:47\").mjd - 8.632259375000002/24/3600, format='mjd').isot.replace(\" \", \"T\") # hard x-ray\n",
    "# t0_utc=\"2022-10-14T19:21:47\"\n",
    "rt=1\n",
    "# nrt=1\n",
    "arc=0\n",
    "required_completeness=0.6\n",
    "mode=\"scw\" # scw|rt|arc\n",
    "global_snr_threshold=3.\n",
    "negative_excesses=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf09ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:43.875917Z",
     "iopub.status.busy": "2023-05-18T15:53:43.875493Z",
     "iopub.status.idle": "2023-05-18T15:53:43.880338Z",
     "shell.execute_reply": "2023-05-18T15:53:43.879350Z"
    },
    "papermill": {
     "duration": 0.019284,
     "end_time": "2023-05-18T15:53:43.882921",
     "exception": false,
     "start_time": "2023-05-18T15:53:43.863637",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "t0_utc = \"2023-05-18T12:59:08.000000\"\n",
    "rt = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5e6777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:43.910053Z",
     "iopub.status.busy": "2023-05-18T15:53:43.909232Z",
     "iopub.status.idle": "2023-05-18T15:53:43.918264Z",
     "shell.execute_reply": "2023-05-18T15:53:43.916636Z"
    },
    "papermill": {
     "duration": 0.022861,
     "end_time": "2023-05-18T15:53:43.921555",
     "exception": false,
     "start_time": "2023-05-18T15:53:43.898694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-18T12:59:08.000000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43c88d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:43.945805Z",
     "iopub.status.busy": "2023-05-18T15:53:43.944850Z",
     "iopub.status.idle": "2023-05-18T15:53:43.952646Z",
     "shell.execute_reply": "2023-05-18T15:53:43.951241Z"
    },
    "papermill": {
     "duration": 0.021793,
     "end_time": "2023-05-18T15:53:43.955057",
     "exception": false,
     "start_time": "2023-05-18T15:53:43.933264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == \"scw\":\n",
    "    rt=0\n",
    "    nrt=1\n",
    "    arc=0\n",
    "elif mode == \"rt\":\n",
    "    rt=1\n",
    "    nrt=0\n",
    "    arc=0\n",
    "elif mode == \"arc\":\n",
    "    rt=0\n",
    "    nrt=0\n",
    "    arc=1\n",
    "elif mode == \"flags\":\n",
    "    print(\"mode set by flags\")\n",
    "else:\n",
    "    raise Exception(\"unknown mode: {}, allowed: scw, rt\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0441a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:43.980299Z",
     "iopub.status.busy": "2023-05-18T15:53:43.979553Z",
     "iopub.status.idle": "2023-05-18T15:53:43.985771Z",
     "shell.execute_reply": "2023-05-18T15:53:43.984744Z"
    },
    "papermill": {
     "duration": 0.019987,
     "end_time": "2023-05-18T15:53:43.988680",
     "exception": false,
     "start_time": "2023-05-18T15:53:43.968693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_coord = SkyCoord(RA, Dec, unit = \"deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc17a03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:44.012471Z",
     "iopub.status.busy": "2023-05-18T15:53:44.011985Z",
     "iopub.status.idle": "2023-05-18T15:53:44.016322Z",
     "shell.execute_reply": "2023-05-18T15:53:44.015425Z"
    },
    "papermill": {
     "duration": 0.019037,
     "end_time": "2023-05-18T15:53:44.020002",
     "exception": false,
     "start_time": "2023-05-18T15:53:44.000965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04258932",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00203fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T15:53:44.044243Z",
     "iopub.status.busy": "2023-05-18T15:53:44.043851Z",
     "iopub.status.idle": "2023-05-18T15:53:44.826791Z",
     "shell.execute_reply": "2023-05-18T15:53:44.825776Z"
    },
    "papermill": {
     "duration": 0.796908,
     "end_time": "2023-05-18T15:53:44.828590",
     "exception": true,
     "start_time": "2023-05-18T15:53:44.031682",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'integralenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# /home/savchenk/work/transients/workflows/integral-all-sky\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mintegralenv\u001b[39;00m\n\u001b[1;32m      4\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(integralenv)\n\u001b[1;32m      6\u001b[0m arc_root_prefix \u001b[38;5;241m=\u001b[39m integralenv\u001b[38;5;241m.\u001b[39mget_arc_root_prefix()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'integralenv'"
     ]
    }
   ],
   "source": [
    "# /home/savchenk/work/transients/workflows/integral-all-sky\n",
    "\n",
    "import integralenv\n",
    "importlib.reload(integralenv)\n",
    "\n",
    "arc_root_prefix = integralenv.get_arc_root_prefix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe4fa6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "now_ijd = float(ic.converttime(\"UTC\",time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\"IJD\"))\n",
    "t0_ijd =  float(ic.converttime(\"UTC\",t0_utc,\"IJD\"))\n",
    "\n",
    "tstart_ijd = t0_ijd - tstart_rel_mseconds/24./3600\n",
    "tstop_ijd = t0_ijd + tstop_rel_seconds/24./3600\n",
    "\n",
    "now_ijd, t0_ijd, tstart_ijd, tstop_ijd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce445c7d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lcs={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c8db5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nrt == 1:\n",
    "    import isdcclient\n",
    "\n",
    "    IC = isdcclient.ISDCClient()\n",
    "\n",
    "    \n",
    "    lcs['ACS'] = IC.genlc(\"ACS\", t0_utc, \"%.10lg\"%max(tstart_rel_mseconds,tstop_rel_seconds),format='numpy')\n",
    "    lcs['ACS'][:,1] = 0.05\n",
    "    \n",
    "    #lcs['IBIS/Veto'] = IC.genlc(\"IBIS_VETO\", t0_utc, \"%.10lg\"%max(tstart_rel_mseconds,tstop_rel_seconds),format='numpy')\n",
    "    #lcs['IBIS/Veto'][:,1] = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f8e5e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebin(lc,n,av = False):\n",
    "    if n == 0: return lc\n",
    "    \n",
    "    N=int(lc.shape[0]/n)*n\n",
    "    if av:\n",
    "        return lc[:N].reshape((int(lc.shape[0]/n), n)).mean(1)\n",
    "    else:\n",
    "        return lc[:N].reshape((int(lc.shape[0]/n), n)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d67dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# if rt == 1:\n",
    "\n",
    "#     got_data = False\n",
    "    \n",
    "#     while not got_data:\n",
    "#         current_rev=float(ic.converttime(\"UTC\",t0_utc,\"REVNUM\"))\n",
    "\n",
    "#         print(\"current rev\", current_rev)\n",
    "\n",
    "#         rtdata_roots=[\n",
    "#             '/unsaved/astro/savchenk/dockers/realtimeacs/docker-ibas/spiacs-lcdump',\n",
    "#             '/rtdata',\n",
    "#             '/mnt/sshfs/isdc-in01//unsaved/astro/savchenk/dockers/realtimeacs/docker-ibas/spiacs-lcdump',    \n",
    "#         ]\n",
    "\n",
    "#         for realtime_dump_root in rtdata_roots + [ None ]:\n",
    "#             #print(\"probing\",realtime_dump_root,\"with\",glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\"))\n",
    "#             if realtime_dump_root and len(glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\"))>0:\n",
    "#                 print(\"this\",realtime_dump_root)\n",
    "#                 break\n",
    "\n",
    "#         if not realtime_dump_root:\n",
    "#             raise Exception(\"no realtime archvie found\")\n",
    "\n",
    "#         for rt_fn in reversed(sorted([l for l in glob.glob(realtime_dump_root+\"/lcdump-revol-*.csv\") if \n",
    "#                        float(re.search(\"lcdump-revol-(\\d{4}).*.csv\",l).groups()[0])<=current_rev+1])):\n",
    "\n",
    "#             print(rt_fn)\n",
    "\n",
    "#             rt_lc = np.genfromtxt(rt_fn)\n",
    "\n",
    "#             lcs['ACS']=rt_lc[:,(3,0,2,0)]\n",
    "#             lcs['ACS'][:,1] = 0.05\n",
    "\n",
    "#             first_data = lcs['ACS'][:,0][0]\n",
    "#             last_data = lcs['ACS'][:,0][-1]\n",
    "\n",
    "#             print(\"now\", now_ijd, \n",
    "#                   \"first data in file\", first_data, \n",
    "#                   \"last data\", last_data, \n",
    "#                   \"requested\", t0_ijd, \n",
    "#                   \"have margin\", (last_data-t0_ijd)*24*3600,\"s\",\n",
    "#                   \"data delay\", (now_ijd-last_data)*24*3600,\"s\")       \n",
    "\n",
    "\n",
    "#             if t0_ijd<first_data:\n",
    "#                 print(\"data in the previous file\")\n",
    "#                 continue\n",
    "                \n",
    "\n",
    "#             print(\"margin\",(last_data-now_ijd)*24*3600-tstop_rel_seconds*1.5 + 100)\n",
    "#             if  (last_data-t0_ijd)*24*3600>tstop_rel_seconds*1.5 + 100:                            \n",
    "#                 print(\"this margin is sufficient\")\n",
    "#                 got_data=True\n",
    "#                 break\n",
    "#             else:\n",
    "#                 print(\"this margin is NOT sufficient, waiting\")\n",
    "#             #    if (now_ijd-last_data)*24*3600>1000:\n",
    "#             #        raise RuntimeError('margin insufficent, data too old: no more hope')                \n",
    "\n",
    "#                 time.sleep(30)\n",
    "#                 break\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45286ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = dict()    \n",
    "\n",
    "for n, lc in lcs.items():\n",
    "\n",
    "    try:\n",
    "        rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    m = rel_s>-tstart_rel_mseconds\n",
    "    m &= rel_s<tstop_rel_seconds\n",
    "\n",
    "    print(\"total lc\",lc.shape)\n",
    "    print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "    print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "\n",
    "    lc = lc[m]\n",
    "\n",
    "    b_tb = np.mean(lc[:,1])    \n",
    "\n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "\n",
    "    expected_telapse = tstop_rel_seconds + tstop_rel_seconds    \n",
    "\n",
    "    if len(rel_s) == 0:\n",
    "        telapse = 0\n",
    "        ontime = 0\n",
    "    else:\n",
    "        telapse = rel_s.max() - rel_s.min()\n",
    "        ontime = np.sum(lc[:,1])\n",
    "\n",
    "\n",
    "    print(\"expected telapse\", expected_telapse, \"telapse\", telapse, \"ontime\", ontime)\n",
    "\n",
    "    if ontime / expected_telapse < required_completeness:\n",
    "        raise Exception(\"data not available: exected %.5lg elapsed %.5lg ontime %.5lg\"%(expected_telapse, telapse, ontime))\n",
    "\n",
    "    lc_summary = dict()\n",
    "    summary[n.replace(\"/\",\"_\")]=lc_summary\n",
    "\n",
    "    print(\"size\", lc.shape, rel_s.shape)\n",
    "\n",
    "    if np.sum(m) == 0: continue\n",
    "\n",
    "    pylab.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "    for ascale in [0.05, 0.5, 1, 10]:\n",
    "        summary_scale = dict()\n",
    "        lc_summary[('s_%.5lg'%ascale).replace(\".\",\"_\")] = summary_scale        \n",
    "\n",
    "\n",
    "        print(\"requested scale\",ascale)\n",
    "        print(\"b_tb\",b_tb)\n",
    "\n",
    "        if b_tb>ascale:\n",
    "            ascale = b_tb\n",
    "\n",
    "\n",
    "        nscale = int(ascale/b_tb)\n",
    "        scale=nscale*b_tb\n",
    "\n",
    "        print(\"acceptable, will be\", nscale, scale)\n",
    "\n",
    "        rate = rebin(lc[:,2],nscale,False)/scale\n",
    "        rate_err = rebin(lc[:,2],nscale,False)**0.5/scale\n",
    "\n",
    "        print(\"rebinned to\",rate.shape)\n",
    "\n",
    "        pylab.errorbar(\n",
    "            rebin(rel_s,nscale,True),\n",
    "            rate,\n",
    "            rate_err,\n",
    "            xerr=scale/4.\n",
    "        )\n",
    "\n",
    "        summary_scale['meanrate'] = np.mean(rate)\n",
    "        summary_scale['maxrate'] = np.max(rate)\n",
    "        summary_scale['stdvar'] = np.std(rate)\n",
    "        summary_scale['meanerr'] = np.mean(rate_err**2)**0.5\n",
    "        summary_scale['excvar'] = summary_scale['stdvar']/summary_scale['meanerr']        \n",
    "\n",
    "        summary_scale['maxsnr'] = np.max((rate-np.mean(rate))/rate_err/summary_scale['excvar'])\n",
    "\n",
    "        summary_scale['localfar'] = stats.norm.sf(summary_scale['maxsnr'])*rate.shape[0]\n",
    "\n",
    "        summary_scale['localfar_s'] = stats.norm.isf(summary_scale['localfar']/2.) if summary_scale['localfar']<1 else 0\n",
    "\n",
    "        # add FAR spike here\n",
    "\n",
    "        if 'best' not in lc_summary or summary_scale['localfar_s'] > lc_summary['best']['localfar_s']:\n",
    "            lc_summary['best'] = dict(\n",
    "                localfar_s = summary_scale['localfar_s'],\n",
    "                scale = ascale,\n",
    "            )\n",
    "\n",
    "        print(summary_scale)\n",
    "\n",
    "\n",
    "    #tight_layout()\n",
    "    pylab.grid()\n",
    "\n",
    "    pylab.xlim(-tstart_rel_mseconds, tstop_rel_seconds)\n",
    "    #pylab.axhspan(0,10,alpha=0.2,color=\"red\")\n",
    "    #pylab.axhspan(10,15,alpha=0.2,color=\"green\")\n",
    "    #pylab.axhspan(15,20,alpha=0.2,color=\"blue\")\n",
    "    pylab.ylabel(n+\", count s$^{-1}$\")\n",
    "    #ylim([0,50])\n",
    "    pylab.xlabel(\"seconds since %s (IJD %.10lg)\"%(t0_utc, t0_ijd))\n",
    "\n",
    "    fn=n.replace(\"/\",\"_\") + \"_lc.png\"\n",
    "    pylab.savefig(fn)\n",
    "    print(\"saving as\",fn)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ab06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# below S/N of 4 FAR is determined primarily by poisson, above - by spikes\n",
    "\n",
    "def approx_FAR_spike_hz(snr, scale):    \n",
    "    lim_snr = 2\n",
    "    \n",
    "    spike_rate_snr6 = 60./3600./24.\n",
    "    if scale>=0.1:\n",
    "        spike_rate_snr6*=(scale/0.1)**-1\n",
    "    \n",
    "    \n",
    "    approx_FAR_hz = snr*0 + spike_rate_snr6 * (lim_snr/6.)**-2.7 \n",
    "    \n",
    "    try:\n",
    "        if snr>lim_snr:\n",
    "            approx_FAR_hz = spike_rate_snr6 * (np.abs(snr)/6.)**-2.7\n",
    "    except:\n",
    "        m=snr>lim_snr\n",
    "        approx_FAR_hz[m] = (np.abs(snr[m])/6.)**-2.7 * spike_rate_snr6\n",
    "        \n",
    "\n",
    "    return approx_FAR_hz\n",
    "\n",
    "def approx_FAR_norm_hz(snr, scale_s):\n",
    "    return stats.norm.sf(snr)/scale_s\n",
    "\n",
    "def approx_FAP(snr, t, scale_s):\n",
    "    \n",
    "    try:\n",
    "        t_scaled = t[:]\n",
    "        t_scaled[abs(t)<scale_s]=scale_s\n",
    "    except:\n",
    "        if abs(t)<scale_s:\n",
    "            t_scaled=scale_s\n",
    "        else:\n",
    "            t_scaled=t\n",
    "\n",
    "    approx_FAP = 2 * ( approx_FAR_norm_hz(snr, scale_s) + approx_FAR_spike_hz(snr, scale_s) )  * abs(t_scaled) * (1+np.log( 30/0.1))\n",
    "    \n",
    "    return approx_FAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc462d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pylab.figure()\n",
    "\n",
    "x=np.linspace(-5,10,100)\n",
    "\n",
    "for scale_s in 0.05, 0.1, 1, 10:\n",
    "\n",
    "    c=pylab.plot(x,approx_FAR_norm_hz(x, scale_s), ls='--')\n",
    "    pylab.plot(x,approx_FAR_spike_hz(x, scale_s),c=c[0].get_color(),ls=\":\")\n",
    "    pylab.plot(x,\n",
    "               approx_FAR_spike_hz(x, scale_s) + approx_FAR_norm_hz(x, scale_s),\n",
    "               c=c[0].get_color()\n",
    "              )\n",
    "\n",
    "\n",
    "    pylab.semilogy()\n",
    "\n",
    "pylab.ylim([1e-5, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df4a77",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timescales = sorted(set([0.05*ns for ns in sorted(set(\n",
    "    list(map(int,np.logspace(0,np.log10(20*30),100))) \n",
    "))]  + list(np.linspace(1,31,30*2+1))))\n",
    "timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286dc93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = dict()    \n",
    "all_excesses=[]            \n",
    "\n",
    "best_lc=None\n",
    "\n",
    "for n, lc in lcs.items():\n",
    "    \n",
    "    #rel_s = lc[:,0]\n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    \n",
    "\n",
    "    m = rel_s>-tstart_rel_mseconds\n",
    "    m &= rel_s<tstop_rel_seconds\n",
    "    \n",
    "    print(\"total lc\",lc.shape)\n",
    "    print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "    print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "    \n",
    "    lc = lc[m]\n",
    "   # rel_s = lc[:,0]\n",
    "    \n",
    "    b_tb = np.mean(lc[:,1])    \n",
    "    \n",
    "    rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "    \n",
    "    expected_telapse = tstop_rel_seconds + tstop_rel_seconds    \n",
    "    \n",
    "    if len(rel_s) == 0:\n",
    "        telapse = 0\n",
    "        ontime = 0\n",
    "    else:\n",
    "        telapse = rel_s.max() - rel_s.min()\n",
    "        ontime = np.sum(lc[:,1])\n",
    "        \n",
    "    \n",
    "    print(\"expected telapse\", expected_telapse, \"telapse\", telapse, \"ontime\", ontime)\n",
    "    \n",
    "    if ontime / expected_telapse < required_completeness:\n",
    "        raise Exception(\"data not available: exected %.5lg elapsed %.5lg ontime %.5lg\"%(expected_telapse, telapse, ontime))\n",
    "        \n",
    "    lc_summary = dict()\n",
    "    summary[n.replace(\"/\",\"_\")]=lc_summary\n",
    "    \n",
    "    print(\"size\", lc.shape, rel_s.shape)\n",
    "    \n",
    "    if np.sum(m) == 0: continue\n",
    "    \n",
    "    pylab.figure(figsize=(8,6))\n",
    "    \n",
    "    best_lc_byscale={}\n",
    "\n",
    "        \n",
    "    \n",
    "    #for ascale in [0.05, 0.1, 0.2, 0.5, 1, 2, 10]:\n",
    "    for ascale in timescales:\n",
    "    #for ascale in [0.05*i for i in range(20)] + [0.5*i for i in range(20)] + [15, 20, 25, 30]:\n",
    "    #for ascale in [0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1, 2, 8, 10]:\n",
    "    #for ascale in [1,]:\n",
    "        s_scale_mo = {}\n",
    "        lc_summary[('s_%.5lg'%ascale).replace(\".\",\"_\")] = s_scale_mo    \n",
    "        \n",
    "        \n",
    "        print(\"requested scale\",ascale)\n",
    "#        print(\"b_tb\",b_tb)\n",
    "        \n",
    "        if b_tb>ascale:\n",
    "            ascale = b_tb\n",
    "                    \n",
    "        \n",
    "        nscale = int(round(ascale/b_tb))\n",
    "        scale=nscale*b_tb\n",
    "        \n",
    "#        print(\"acceptable, will be\", nscale, scale)\n",
    "        \n",
    "        c=None\n",
    "        \n",
    "        \n",
    "        #for offset in range(0,nscale):            \n",
    "        #for offset in (, round(nscale/2)):            \n",
    "        \n",
    "        if nscale < 20:\n",
    "            offsets = range(0, round(nscale/2)+1)\n",
    "        else:\n",
    "            offsets = range(0, round(nscale/2)+1, max(round(round(nscale/2)/20), 1))\n",
    "        \n",
    "        for offset in offsets: \n",
    "            summary_scale = dict()\n",
    "            s_scale_mo[offset]=summary_scale\n",
    "            \n",
    "            rel_s_scale = rebin(rel_s[offset:],nscale,True)\n",
    "            rate = rebin(lc[offset:,2],nscale,False)/scale        \n",
    "            rate_err = rebin(lc[offset:,2],nscale,False)**0.5/scale\n",
    "\n",
    "            #print(\"rebinned to\",rate.shape)\n",
    "            \n",
    "            \n",
    "            summary_scale['scale_s']=scale\n",
    "            summary_scale['meanrate'] = np.mean(rate)\n",
    "            summary_scale['maxrate'] = np.max(rate)            \n",
    "            summary_scale['stdvar'] = np.std(rate)\n",
    "            summary_scale['meanerr'] = np.mean(rate_err**2)**0.5\n",
    "            summary_scale['excvar'] = summary_scale['stdvar']/summary_scale['meanerr']        \n",
    "\n",
    "            if negative_excesses==1:\n",
    "                snr = -(rate-np.mean(rate))/rate_err/summary_scale['excvar']\n",
    "            else:\n",
    "                snr = (rate-np.mean(rate))/rate_err/summary_scale['excvar']\n",
    "            \n",
    "            i_max = np.argmax(snr)\n",
    "            \n",
    "            print(i_max,snr[i_max],rel_s_scale[i_max])\n",
    "            \n",
    "            summary_scale['maxsnr'] = snr[i_max]\n",
    "            summary_scale['maxsnr_t'] = rel_s_scale[i_max]\n",
    "\n",
    "            summary_scale['localfar'] = stats.norm.sf(summary_scale['maxsnr'])*rate.shape[0]\n",
    "\n",
    "            summary_scale['localfar_s'] = stats.norm.isf(summary_scale['localfar']/2.) if summary_scale['localfar']<1 else 0\n",
    "            \n",
    "            m_over_threshold = snr > global_snr_threshold\n",
    "                        \n",
    "            excesses = dict(\n",
    "                            snr = snr[m_over_threshold],\n",
    "                            rel_s_scale = rel_s_scale[m_over_threshold],\n",
    "                            rate = rate[m_over_threshold],\n",
    "                            rate_err = rate_err[m_over_threshold],\n",
    "                            rate_overbkg = rate[m_over_threshold] - np.mean(rate),\n",
    "                        )\n",
    "                        \n",
    "            summary_scale['excesses'] = [dict(zip(excesses.keys(), er)) for er in zip(*excesses.values())]\n",
    "\n",
    "            for e in summary_scale['excesses']:\n",
    "                e['FAP'] = approx_FAP(e['snr'], e['rel_s_scale'], scale)\n",
    "            \n",
    "            all_excesses+=[\n",
    "                        dict(scale=scale, offset=offset,excess=e) for e in summary_scale['excesses']\n",
    "                    ]\n",
    "\n",
    "            print(\"scale\", scale, \"offset\", offset, \"found excesses\",len(summary_scale['excesses']))\n",
    "            \n",
    "            \n",
    "\n",
    "            #r=pylab.errorbar(\n",
    "            #    rebin(rel_s[offset:],nscale,True),\n",
    "            #    rate,\n",
    "            #    rate_err,\n",
    "            #    xerr=scale/4.,\n",
    "            #    c=c,\n",
    "            #    alpha=0.7\n",
    "            #)\n",
    "            \n",
    "        #    print(rel_s_scale.shape, snr.shape)\n",
    "            \n",
    "            r=pylab.errorbar(\n",
    "                rel_s_scale,\n",
    "                snr,\n",
    "                snr*0+1,\n",
    "                xerr=scale/4.,\n",
    "                c=c,\n",
    "                alpha=0.7\n",
    "            )\n",
    "            \n",
    "            pylab.axvline(summary_scale['maxsnr_t'],c=\"k\")\n",
    "            \n",
    "            \n",
    "            c=r[0].get_color()\n",
    "\n",
    "            \n",
    "            # add FAR spike here\n",
    "\n",
    "            if 'best' not in lc_summary or summary_scale['localfar_s'] > lc_summary['best']['localfar_s']:\n",
    "                lc_summary['best'] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                )\n",
    "                best_lc=rel_s_scale,rate,rate_err\n",
    "            \n",
    "            if 'best' not in s_scale_mo or summary_scale['localfar_s'] > s_scale_mo['best']['localfar_s']:\n",
    "                s_scale_mo['best'] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                )\n",
    "              #  best_lc=rel_s_scale,rate,rate_err\n",
    "            \n",
    "            if ascale not in best_lc_byscale or summary_scale['localfar_s'] > best_lc_byscale[ascale]['localfar_s']:\n",
    "                best_lc_byscale[ascale] = dict(\n",
    "                    localfar_s = summary_scale['localfar_s'],\n",
    "                    scale = ascale,\n",
    "                    summary_scale = summary_scale,\n",
    "                    best_lc=(rel_s_scale,rate,rate_err),\n",
    "                )\n",
    "                \n",
    "        \n",
    "            #print(summary_scale)\n",
    "        s_scale_mo.update(s_scale_mo['best']['summary_scale'])\n",
    "\n",
    "    #tight_layout()\n",
    "    pylab.grid()\n",
    "\n",
    "    #pylab.xlim(-tstart_rel_mseconds, tstop_rel_seconds)\n",
    "    #pylab.axhspan(0,10,alpha=0.2,color=\"red\")\n",
    "    #pylab.axhspan(10,15,alpha=0.2,color=\"green\")\n",
    "    #pylab.axhspan(15,20,alpha=0.2,color=\"blue\")\n",
    "    pylab.ylabel(n+\", S/N\")\n",
    "    #ylim([0,50])\n",
    "    pylab.xlabel(\"seconds since %s (IJD %.10lg)\"%(t0_utc, t0_ijd))\n",
    "    \n",
    "    detfn=n.replace(\"/\",\"_\") + \"_det_lc.png\"\n",
    "    pylab.savefig(detfn)\n",
    "    print(\"saving as\",detfn)\n",
    "    \n",
    "summary['ACS']['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f1f05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_excesses = []\n",
    "\n",
    "for i in sorted(all_excesses, key=lambda x:x['excess']['FAP']):\n",
    "    if i['excess']['FAP']<1 or True:\n",
    "        print(i['scale'],i['offset'], i['excess']['snr'], i['excess']['rel_s_scale'], i['excess']['FAP'])\n",
    "        \n",
    "        grouped=False\n",
    "        for g in grouped_excesses:\n",
    "            if abs(i['excess']['rel_s_scale']-g['excess']['rel_s_scale'])<max(i['scale'],g['scale']):\n",
    "                print(\"to group\", g['excess']['rel_s_scale'])\n",
    "                if i['excess']['snr'] > g['excess']['snr']:\n",
    "                    print(\"group takeover\")\n",
    "                    g.update(i)\n",
    "                grouped=True\n",
    "                \n",
    "        if not grouped:\n",
    "            print(\"new group\")\n",
    "            #i['group']=[i]\n",
    "            grouped_excesses.append(i)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2c8ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_excesses=sorted(grouped_excesses, key=lambda x:x['excess']['FAP'])\n",
    "\n",
    "for i in grouped_excesses:\n",
    "    print(\"%4.2lg\"%i['scale'], \"%5.2lg\"%i['excess']['snr'], \"%6.3lg\"%i['excess']['rel_s_scale'], \"%7.5lg\"%i['excess']['FAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dee1c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "len(json.dumps(grouped_excesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd5f73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary['ACS']['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bf196",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary['ACS']['s_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99774c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "excvar_summary=dict(\n",
    ")\n",
    "\n",
    "for k,s in summary['ACS'].items():\n",
    "    if 'scale_s' in s:\n",
    "        print(\"%.5lg\"%s['scale_s'], \"%5.4lg\"%s['excvar'])\n",
    "        \n",
    "        if s['scale_s']<=0.200:\n",
    "            kg='hf_200ms'\n",
    "        elif s['scale_s']<=2.00:\n",
    "            kg='mf_200ms_2s'\n",
    "        elif s['scale_s']<=10.00:\n",
    "            kg='mf_2s_10s'\n",
    "        else:\n",
    "            kg='lf_10s'\n",
    "        \n",
    "        if kg not in excvar_summary:\n",
    "            excvar_summary[kg]=[s['excvar']]\n",
    "        else:\n",
    "            excvar_summary[kg]+=[s['excvar']]\n",
    "\n",
    "for k,v in excvar_summary.items():\n",
    "    print(k,min(v),max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ef9df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig_names=[]\n",
    "\n",
    "\n",
    "for limit_group in 0.02, 0.1, 1: \n",
    "    figs=dict()\n",
    "\n",
    "    for n, lc in lcs.items():\n",
    "        rel_s = (lc[:,0]-t0_ijd)*24*3600\n",
    "\n",
    "        m = rel_s>-tstart_rel_mseconds\n",
    "        m &= rel_s<tstop_rel_seconds\n",
    "\n",
    "        print(\"total lc\",lc.shape)\n",
    "        print(\"min\",lc[:,0].min()-t0_ijd)\n",
    "        print(\"max\",lc[:,0].max()-t0_ijd)\n",
    "\n",
    "        lc = lc[m]\n",
    "        rel_s = rel_s[m]\n",
    "\n",
    "        for excess in grouped_excesses:\n",
    "            #if excess['excess']['FAP'] > 0.02: continue\n",
    "            if excess['excess']['FAP'] > limit_group: continue\n",
    "\n",
    "            print(excess)\n",
    "\n",
    "\n",
    "            offset = excess['offset']\n",
    "            nscale = int(excess['scale']/b_tb)  \n",
    "            scale=excess['scale']\n",
    "\n",
    "            s_figs = sorted(figs.items(), key=lambda x:abs(x[0]-scale))\n",
    "\n",
    "            if len(s_figs) == 0 or s_figs[0][0] < scale*0.5 or s_figs[0][0] > scale*1.5: \n",
    "                fig = pylab.figure(figsize=(8,6))\n",
    "                figs[scale] = fig\n",
    "                pylab.xlim([-2,2])\n",
    "                pylab.xlabel(\"seconds since \"+t0_utc)\n",
    "                pylab.ylabel(\"counts/s\")\n",
    "                pylab.title(\"FAP threshold %.5lg\"%limit_group)\n",
    "            else:            \n",
    "                print(\"good match\", s_figs[0][0], scale)\n",
    "                pylab.figure(s_figs[0][1].number)\n",
    "                pylab.xlabel(\"seconds since \"+t0_utc)\n",
    "                pylab.ylabel(\"counts/s\")\n",
    "\n",
    "\n",
    "\n",
    "            rel_s_scale = rebin(rel_s[offset:],nscale,True)\n",
    "            rate = rebin(lc[offset:,2],nscale,False)/scale        \n",
    "            rate_err = rebin(lc[offset:,2],nscale,False)**0.5/scale\n",
    "\n",
    "            bkg=np.mean(rate)\n",
    "\n",
    "            m_on = np.abs(rel_s_scale-excess['excess']['rel_s_scale'])<excess['scale']*1.5\n",
    "\n",
    "\n",
    "            pylab.grid(False)\n",
    "\n",
    "            pylab.axhline(0, alpha=0.2, ls=\":\", color='gray')\n",
    "\n",
    "            cr=pylab.errorbar(\n",
    "                    rel_s_scale, \n",
    "                    (rate-bkg), \n",
    "                    (rate_err),\n",
    "                    alpha=0.5,\n",
    "                    ls=\"\",\n",
    "                )[0].get_color()\n",
    "\n",
    "            pylab.step(\n",
    "                    rel_s_scale, \n",
    "                    (rate-bkg), \n",
    "              #      (rate_err),\n",
    "                    alpha=0.5,\n",
    "                    where=\"mid\",\n",
    "                    c=cr,\n",
    "                )\n",
    "\n",
    "            pylab.axhline(np.std(rate)*3, alpha=0.2, ls=\"--\",c=cr)\n",
    "            pylab.axhline(np.std(rate)*5, alpha=0.2, ls=\"--\", lw=2,c=cr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pylab.errorbar(\n",
    "                    rel_s_scale[m_on], \n",
    "                    (rate-bkg)[m_on], \n",
    "                    (rate_err)[m_on],\n",
    "                    lw=2.,\n",
    "                    alpha=1,\n",
    "                    label=\"S/N %.3lg FAP %.3lg scale %.3lg s\"%(excess['excess']['snr'],excess['excess']['FAP'],excess['scale']),\n",
    "                    c=cr\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "            newlim=([\n",
    "                min([excess['excess']['rel_s_scale']*1.3-excess['scale']*5,-excess['scale']*5]),\n",
    "                max([excess['excess']['rel_s_scale']*1.3+excess['scale']*5,excess['scale']*5]),\n",
    "            ])\n",
    "\n",
    "            oldlim=pylab.gca().get_xlim()\n",
    "\n",
    "            print(oldlim)\n",
    "\n",
    "            pylab.xlim([\n",
    "                min([oldlim[0],newlim[0]]),\n",
    "                max([oldlim[1],newlim[1]]),\n",
    "            ])\n",
    "\n",
    "\n",
    "    for f_i,(s,f) in enumerate(figs.items()):\n",
    "        f.legend()\n",
    "        f.gca().axvline(0,ls=\"--\",c=\"r\",lw=3)\n",
    "        fn=\"excess_%.5lg_%i.png\"%(s,len(fig_names))\n",
    "        f.savefig(fn)\n",
    "        fig_names.append(fn)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6272c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols=1\n",
    "rows=int(np.ceil(len(fig_names)/cols))\n",
    "\n",
    "\n",
    "if rows>0:\n",
    "    f, axes=pylab.subplots(rows, cols, figsize=(12, 8*rows))\n",
    "    print(\"axes\",axes,axes.__class__)\n",
    "\n",
    "    if rows>1:\n",
    "        axes=axes.flatten()\n",
    "    else:\n",
    "        axes=[axes]\n",
    "\n",
    "    for i,fn in enumerate(fig_names):\n",
    "        #f.add_subplot(len(fig_names), 2, i+1)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].imshow(pylab.imread(fn) ) #, extent=(0,1,0,1))\n",
    "        #pylab.imshow(pylab.imread(fn), extent=(0,1,(i-1)/len(fig_names),i/len(fig_names)))\n",
    "\n",
    "    f.tight_layout()\n",
    "else:\n",
    "    f=pylab.figure()\n",
    "\n",
    "f.savefig(\"excesses_mosaic.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0236002",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if rt == 1:    \n",
    "    summary['ACS_rt'] = summary['ACS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702751eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "summary['ACS']['s_1']['meanerr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a66327",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "outputs"
    ]
   },
   "outputs": [],
   "source": [
    "acs_lc_png=\"ACS_lc.png\"\n",
    "acs_rt_lc_png=\"ACS_lc.png\"\n",
    "acs_rt_det_lc_png=\"ACS_det_lc.png\"\n",
    "ibis_veto_lc_png=\"IBIS_Veto_lc.png\"\n",
    "excesses_mosaic_png=\"excesses_mosaic.png\"\n",
    "summary=summary\n",
    "reportable_excesses=grouped_excesses\n",
    "excvar_summary=excvar_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b093f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "injected-gather-outputs"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import base64\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "    \n",
    "from nb2workflow.nbadapter import denumpyfy\n",
    "from nb2workflow.json import CustomJSONEncoder\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"acs_lc_png\",denumpyfy(acs_lc_png))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue acs_lc_png\", acs_lc_png)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"acs_lc_png\",json.dumps(denumpyfy(acs_lc_png), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(acs_lc_png,str) and os.path.exists(acs_lc_png):\n",
    "    variable_name = \"acs_lc_png\"\n",
    "    fn = acs_lc_png\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"acs_lc_png_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"acs_rt_lc_png\",denumpyfy(acs_rt_lc_png))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue acs_rt_lc_png\", acs_rt_lc_png)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"acs_rt_lc_png\",json.dumps(denumpyfy(acs_rt_lc_png), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(acs_rt_lc_png,str) and os.path.exists(acs_rt_lc_png):\n",
    "    variable_name = \"acs_rt_lc_png\"\n",
    "    fn = acs_rt_lc_png\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"acs_rt_lc_png_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"acs_rt_det_lc_png\",denumpyfy(acs_rt_det_lc_png))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue acs_rt_det_lc_png\", acs_rt_det_lc_png)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"acs_rt_det_lc_png\",json.dumps(denumpyfy(acs_rt_det_lc_png), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(acs_rt_det_lc_png,str) and os.path.exists(acs_rt_det_lc_png):\n",
    "    variable_name = \"acs_rt_det_lc_png\"\n",
    "    fn = acs_rt_det_lc_png\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"acs_rt_det_lc_png_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"ibis_veto_lc_png\",denumpyfy(ibis_veto_lc_png))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue ibis_veto_lc_png\", ibis_veto_lc_png)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"ibis_veto_lc_png\",json.dumps(denumpyfy(ibis_veto_lc_png), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(ibis_veto_lc_png,str) and os.path.exists(ibis_veto_lc_png):\n",
    "    variable_name = \"ibis_veto_lc_png\"\n",
    "    fn = ibis_veto_lc_png\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"ibis_veto_lc_png_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"excesses_mosaic_png\",denumpyfy(excesses_mosaic_png))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue excesses_mosaic_png\", excesses_mosaic_png)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"excesses_mosaic_png\",json.dumps(denumpyfy(excesses_mosaic_png), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(excesses_mosaic_png,str) and os.path.exists(excesses_mosaic_png):\n",
    "    variable_name = \"excesses_mosaic_png\"\n",
    "    fn = excesses_mosaic_png\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"excesses_mosaic_png_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"summary\",denumpyfy(summary))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue summary\", summary)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"summary\",json.dumps(denumpyfy(summary), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(summary,str) and os.path.exists(summary):\n",
    "    variable_name = \"summary\"\n",
    "    fn = summary\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"summary_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"reportable_excesses\",denumpyfy(reportable_excesses))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue reportable_excesses\", reportable_excesses)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"reportable_excesses\",json.dumps(denumpyfy(reportable_excesses), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(reportable_excesses,str) and os.path.exists(reportable_excesses):\n",
    "    variable_name = \"reportable_excesses\"\n",
    "    fn = reportable_excesses\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"reportable_excesses_url\", url)\n",
    "\n",
    "\n",
    "try:\n",
    "    sb.glue(\"excvar_summary\",denumpyfy(excvar_summary))\n",
    "except Exception as e:\n",
    "    print(\"failed to glue excvar_summary\", excvar_summary)\n",
    "    print(\"will glue jsonified\")\n",
    "    sb.glue(\"excvar_summary\",json.dumps(denumpyfy(excvar_summary), cls=CustomJSONEncoder))\n",
    "\n",
    "if isinstance(excvar_summary,str) and os.path.exists(excvar_summary):\n",
    "    variable_name = \"excvar_summary\"\n",
    "    fn = excvar_summary\n",
    "    content = open(fn ,'rb').read()    \n",
    "\n",
    "    if None is None or len(content) < None:\n",
    "        encoded = base64.b64encode(content).decode()\n",
    "        print(\"glueing file\", fn)\n",
    "        sb.glue(variable_name + \"_content\", encoded)\n",
    "    else:\n",
    "        # TODO: make a customizable upload to different DL platforms; before that it should be enabled with caution    \n",
    "        os.makedirs(\"/tmp/nb2w-store\", exist_ok=True)\n",
    "        url = \"file:///tmp/nb2w-store/\" + str(hashlib.md5(content).hexdigest())\n",
    "        print(\"storing file to URL\", url)\n",
    "        with open(url.replace(\"file://\", \"\"), \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sb.glue(\"excvar_summary_url\", url)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.844671,
   "end_time": "2023-05-18T15:53:45.660553",
   "environment_variables": {},
   "exception": true,
   "input_path": "/tmp/nb2w-oa5_g1p8/integralallsky_preproc.ipynb",
   "output_path": "/tmp/nb2w-oa5_g1p8/integralallsky_output.ipynb",
   "parameters": {
    "rt": 1,
    "t0_utc": "2023-05-18T12:59:08.000000"
   },
   "start_time": "2023-05-18T15:53:36.815882",
   "version": "2.4.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "1892832a55cb9b21d693a1e0afb7563f6ad424a1b7b3da21728b3ea0874fe733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}